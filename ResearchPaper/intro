The exponential growth of scientific and business data has resulted in the evolution of the cloud computing . The Cloud computing has gained rapid popularity from quite some time and it's usage has been quadrupled in the last 4 years. According to a research by Nasuni in 2013 there was just over 1 Exabyte or 1024 Petabytes of data stored in the cloud, and now  Google Cloud Storge alone has about 30 Exabytes of data and While there exists even bigger gaints in the industry like the Amazon Web Services (AWS) , Microsoft Azure and many more equally efficient cloud storage providers like the Alibaba Cloud , IBM Cloud ..etcetera etcetera.. . But what exactly is the cloud storage and processing used for? The cloud companies currenty provide the resources the data-intensive computing needs which includes advertising optimizations, user interest predictions, mail anti-spam detection and many such similar data analytics it is also currenty being used for live streaming of data which turned out to be the main reason for the success story of one of the most famous media-services provider, the Netflix.

The implementation that the current cloud providers have adoped to structure their datacenters for the best effeciency of data storage/retrival is the HDFS architecture i.e ,the Hadoop Distributed File System and parallel processing of data for the effective processing of data. However, this implementation faces tremendous energy consumption, carbon dioxide emission and associated costs concerns. With energy consumption becoming key issue for the operation and maintenance of cloud datacenters, cloud computing providers are becoming profoundly concerned .So most of the cloud enterprises today are focusing their attention on energy efficient computing, motivated by high operational costs for their large scale clusters and warehouses. This power related cost includes investment, operating expenses, cooling costs and environmental impacts.

HDFS
====

Hadoop Distributed File System briefly know as HDFS is a key part of the many cloud eco systems, as it provides a reliable means for managing pools of big data and supporting related big data analytics applications.HDFS supports the rapid transfer of data between compute nodes.When HDFS takes in data, it breaks the information down into separate blocks and distributes them to different nodes in a cluster, thus enabling highly efficient parallel processing.Moreover, the Hadoop Distributed File System is specially designed to be highly fault-tolerant. The file system replicates, or copies, each piece of data multiple times and distributes the copies to individual nodes, placing at least one copy on a different server rack than the others. As a result, the data on nodes that crash can be found elsewhere within a cluster. This ensures that processing can continue while data is recovered.HDFS uses master/slave architecture. In its initial incarnation, each Hadoop cluster consisted of a single NameNode that managed file system operations and supporting DataNodes that managed data storage on individual compute nodes.
"""The Hadoop Distributed File System arose at Yahoo as a part of that company's ad serving and search engine requirements. Like other web-oriented companies, Yahoo found itself juggling a variety of applications that were accessed by a growing numbers of users, who were creating more and more data. Facebook, eBay, LinkedIn and Twitter are among the web companies that used HDFS to underpin big data analytics to address these same requirements.But the file system found use beyond that. HDFS was used by The New York Times as part of large-scale image conversions, Media6Degrees for log processing and machine learning, LiveBet for log storage and odds analysis"""
Because HDFS is typically deployed as part of very large-scale implementations, support for low-cost commodity hardware is a particularly useful feature. Such systems, running web search and related applications, for example, can range into the hundreds of petabytes and thousands of nodes. They must be especially resilient, as server failures are common at such scale.

